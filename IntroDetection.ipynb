{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Short Intro-detection from tv-series"
      ],
      "metadata": {
        "id": "HRUxYx4TAXjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem definition\n"
      ],
      "metadata": {
        "id": "-PvKTwazAjf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will represent our video as a sequence of 1 fps images and then applying binary classification to each frame: $$\\lbrace 0, 0, 0 , 1, 1, 1, 1, ...., 0, 0, 0 \\rbrace$$\n",
        "where $1$ represents as intro frame, and $0$ - indicates the main video\n",
        "\n",
        "Such approach allows for model to remain independent from video duration"
      ],
      "metadata": {
        "id": "FESrhO9gAoMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "snh4YCH1CX6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Input\n"
      ],
      "metadata": {
        "id": "4dAWbwDWDLr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model processes video content as sliding windows of 60 consecutive\n",
        "frames, sampled at a rate of 1 FPS. Each frame is resized to 224×224 pixels\n",
        "and normalized using standard ImageNet statistics. The resulting input tensor\n",
        "has the shape (B, T, C, H, W), where B is the batch size, T = 60 is the temporal\n",
        "window length, C = 3 is the number of color channels, and H, W = 224 are the\n",
        "spatial dimensions.\n"
      ],
      "metadata": {
        "id": "xipWSrtPDTlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extractor\n"
      ],
      "metadata": {
        "id": "WHMZE2XRDb6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each frame is passed through the CLIP image encoder, producing a **512-dimensional embedding**:\n",
        "$$f_t = CLIP(I_t)  \\ \\forall t \\in \\left[1, 60\\right]$$\n",
        "Where $I_t$ represents as frme at time step $t$.\n",
        "\n",
        "The output feature sequence:\n",
        "$$\\left( B, T, D\\right)$$\n",
        "Where B - batch-size, $T = 60$, $D = 512$"
      ],
      "metadata": {
        "id": "DKorSi09DgS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Positional encoding"
      ],
      "metadata": {
        "id": "Y3NuCYLLFLN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To preserve temporal structure, we incorporate positional encodings into CLIP embeddings.\n",
        " The final embedding matrix is obtained as:\n",
        " $$E = \\left[f_1+ P_1, ..., f_{60}+P_{60} \\right]$$\n",
        "Where $P_t$ represents the learnable positional encoding at timestep $t$. This ensures\n",
        "that the model learns relative temporal dependencies within the input sequence.\n"
      ],
      "metadata": {
        "id": "WxTCrz2KFOwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multihead Attention for Temporal Context"
      ],
      "metadata": {
        "id": "G2LNtVOgHjvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To capture long-range dependencies between frames, we employ a **multihead\n",
        "attention mechanism**. The attention module consists of **16 heads and 16\n",
        "transformer layers**, allowing the model to:\n",
        "* Learn contextual dependencies between frames.\n",
        "* Recognize patterns in intros and credits that span multiple frames.\n",
        "* Differentiate between fast and slow transitions, improving robustness\n",
        "across different editing styles.\n",
        "\n",
        "Each attention head computes a weighted sum of input embeddings:\n",
        "\n",
        "Attention(Q, K, V) = softmax($\\frac{QK^T}{\\sqrt{d_k}}$) V\n",
        "\n",
        "where Q, K, V are the query, key, and value matrices derived from input embeddings."
      ],
      "metadata": {
        "id": "ZS7bCXzeHotE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frame-wise Classification"
      ],
      "metadata": {
        "id": "F83ANoGwJGsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final classification layer consists of 60 independent linear classifiers,\n",
        "where each classifier processes a single frame in the sequence and predicts\n",
        "whether it belongs to an intro/credit or main content. The output is represented as:\n",
        "$$ \\hat y_t = σ(W_t E_t + b_t) \\ ∀ t \\in \\left[ 1, 60 \\right]$$\n",
        "where $W_t$ and $b_t$ are the parameters of the classifier at timestep t, and σ denotes\n",
        "the sigmoid activation function\n",
        "\n",
        "The predictions from all 60 classifiers are concatenated to form the final\n",
        "sequence output:\n",
        "$$ \\hat Y = \\left[ \\hat y_1, ..., \\hat y_{60}\\right]$$\n",
        "which is then used for sequence labeling"
      ],
      "metadata": {
        "id": "0gfjY9ntJOas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model implementation"
      ],
      "metadata": {
        "id": "2LuFZd2KKKfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import open_clip\n",
        "from tqdm import tqdm\n",
        "\n",
        "class IntroDetector(nn.Module):\n",
        "    def __init__(self,clip_name = \"ViT-B-32\", window_size = 60,transformer_layers = 16,n_heads = 16,dropout = 0.1,unfreeze_clip= False):\n",
        "        super().__init__()\n",
        "        # CLIP\n",
        "        self.clip, _, self.clip_preprocess = open_clip.create_model_and_transforms(clip_name, pretrained=\"laion2b_s34b_b79k\")\n",
        "        self.clip.eval()\n",
        "        if not unfreeze_clip:\n",
        "            for p in self.clip.parameters():\n",
        "                p.requires_grad_(False)\n",
        "        self.embed_dim = self.clip.visual.output_dim  # 512 for ViT‑B/32\n",
        "        #PE\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, window_size, self.embed_dim) * 0.02)\n",
        "        # Transformer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=self.embed_dim,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=self.embed_dim * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=transformer_layers)\n",
        "        #Classification head\n",
        "        self.head = nn.Linear(self.embed_dim, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.flatten(0, 1)  # (B*T,3,224,224)\n",
        "        with torch.set_grad_enabled(self.training and any(p.requires_grad for p in self.clip.parameters())):\n",
        "            feats = self.clip.encode_image(x)  # (B*T, D)\n",
        "        feats = feats.view(B, T, -1)\n",
        "        feats = feats + self.pos_embed[:, :T]\n",
        "        feats = self.transformer(feats)\n",
        "        logits = self.head(feats).squeeze(-1)  # (B,T)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Td7rCU1fLLlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation metrics\n"
      ],
      "metadata": {
        "id": "ridpUTOxMLEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assess model performance, we report accuracy, precision, recall, and F1-\n",
        "score — the most commonly used metrics in binary classification tasks. Accuracy reflects the overall proportion of correctly labeled frames. Precision\n",
        "indicates how many of the predicted intro/credit frames are actually correct,\n",
        "while recall measures how many of the true intro/credit frames were successfully identified. The F1-score summarizes both by computing their harmonic\n",
        "mean."
      ],
      "metadata": {
        "id": "w8lHL60lMTJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    for frames, labels in loader:\n",
        "        frames, labels = frames.to(device), labels.to(device)\n",
        "        logits = model(frames)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs > 0.5).float()\n",
        "        y_true.append(labels.cpu().numpy())\n",
        "        y_pred.append(preds.cpu().numpy())\n",
        "    y_true = np.concatenate(y_true).ravel()\n",
        "    y_pred = np.concatenate(y_pred).ravel()\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return {\"acc\": acc, \"prec\": precision, \"rec\": recall, \"f1\": f1}"
      ],
      "metadata": {
        "id": "oPJnLTY6M35N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}